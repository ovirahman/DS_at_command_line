#!/bin/bash

# This repository includes code adapted from Data Science at the Command Line by Jeroen Janssens, used under the MIT License.
# https://github.com/jeroenjanssens/data-science-at-the-command-line

## shell function

fac() { (echo 1; seq $1) | paste -s -d\* - | bc; }

    # (echo 1; seq $1): generates numbers from 1 to $1, starting with 1
    # paste -s -d'*' - : combines all numbers into a single multiplication expression
    # bc: evaluates the expression


# pipe
curl -s "https://www.gutenberg.org/files/11/11-0.txt" | grep " CHAPTER"

    # This script downloads the full text of "Alice’s Adventures in Wonderland"
    # from Project Gutenberg and lists all chapter headings.

    # Step 1: Use curl to silently (-s) download the book from the URL
    # Step 2: Pipe the content into grep to filter and print lines containing " CHAPTER"
    #         The leading space in " CHAPTER" helps avoid unrelated matches.


# wc 
curl -s "https://www.gutenberg.org/files/11/11-0.txt" | grep " CHAPTER" | wc -l
    # If no file is specified, wc reads from standard input.
    # Options
    # -l: Counts the number of lines.
    # -w: Counts the number of words.
    # -c: Counts the number of bytes.
    # -m: Counts the number of characters.
    # -L: Prints the length of the longest line.


# Redirect the output
curl "https://www.gutenberg.org/files/11/11-0.txt" | grep " CHAPTER" > chapters.txt
    # Redirect the output to a file named chapters.txt


# You can also append the output to a file with >>, meaning the output is added after the original contents:
echo -n "Hello" > greeting.txt
echo " World" >> greeting.txt


# count
cat greeting.txt | wc -w

# can be achieved by using the smaller-than-sign (<)
< greeting.txt wc -w
    # directly passing the file to the standard input of wc without running an additional process

# wc multiple arguments
wc -w greeting.txt movies.txt


#redirect standard error to /dev/null as follows:
cat movies.txt 404.txt 2> /dev/null


dseq 5 > dates.txt
< dates.txt nl | sponge dates.txt
bat dates.txt
    # '< dates.txt': reads from 'dates.txt' using input redirection
    # 'nl': numbers the lines from the input
    # 'sponge dates.txt': temporarily soaks up the input and then writes it back to 'dates.txt'
    #    This prevents the file from being truncated before the command finishes
    #    (a common issue with shell redirection when writing to the same file you're reading from)


#mv
mv ../../greeting.txt hello.txt


# cp (copy)
cp hello.txt bye.txt
rm bye.txt 

# If you want to remove an entire directory with all its contents, specify the -r option, which stands for recursive:
rm -r /data/ch02/old

#All of the above command-line tools accept the -v option, which stands for verbose, so that they output what’s going on. For example:

mkdir -v backup


#All tools but mkdir also accept the -i option, which stands for interactive, and causes the tools to ask you for confirmation.
rm -i *
    # zsh: sure you want to delete all 12 files in /data [yn]? n


# trim with pipe
cat ../ch07/tips.csv | trim 5 25


# tee
seq 0 2 100 | tee even.txt | trim 5
    # Step 1: Generate even numbers from 0 to 100 with a step of 2
    # Step 2: `tee even.txt` writes output to the file AND passes it downstream
    # Step 3: `trim 5` is assumed to be a command that keeps only the first 5 lines
    #         If 'trim' is not installed, replace with `head -n 5`


# help manual
man tar | trim 20
man zshbuiltins | trim
jq --help | trim

# tldr
tldr tar | trim 20

##############################

# chapter 3

# curl and save
curl -s "https://en.wikipedia.org/wiki/List_of_windmills_in_Friesland" -O
curl -s "https://en.wikipedia.org/wiki/List_of_windmills_in_Friesland" > friesland.html
curl -s "dict://dict.org/d:windmill" | trim
curl -s "https://bit.ly/2XBxvwK"
curl -s "https://youtu.be/dQw4w9WgXcQ" | trim
curl -sI "https://youtu.be/dQw4w9WgXcQ" | trim


# decompress with tar
tar -xzf logs.tar.gz
    # This will unpack the logs.tar.gz file into the current directory
    # Command breakdown:
    # tar      : archive utility for creating/extracting tarballs
    # -x       : extract files from the archive
    # -z       : filter the archive through gzip (for .gz files)
    # -f       : specifies the name of the tar file to operate on

tar -tzf logs.tar.gz | trim
    # Command breakdown:
    # tar -t    : list the contents of the archive
    # -z        : filter through gzip (.gz support)
    # -f        : use the file named logs.tar.gz
    # | trim    : pass the list to a command called 'trim' (assumed to be installed)


# mkdir and extract those files there using the -C option.

mkdir logs
 
tar -xzf logs.tar.gz -C logs



# csv
csvlook tmnt-with-header.csv
 #from csvkit

csvlook -H tmnt-missing-newline.csv 
    #The -H option specifies that the CSV file has no header

# excel to csv
curl https://cms-assets.nporadio.nl/npoRadio2/TOP-2000-2020.xlsx > top2000.xlsx
 
in2csv top2000.xlsx | tee top2000.csv | trim

csvgrep top2000.csv --columns ARTIEST --regex '^Queen$' | csvlook -I


in2csv --names top2000.xlsx
    # prints the names of all the worksheets


#Query relational database
sql2csv --db 'sqlite:///r-datasets.db' \> --query 'SELECT row_names AS car, mpg FROM mtcars ORDER BY mpg' | csvlook


# Calling Web APIs
curl -s "https://anapioficeandfire.com/api/characters/583" | jq '.'
    # curl -s   : silently fetch data from a URL (no progress or errors shown)
    # "https://anapioficeandfire.com/api/characters/583" : API endpoint for Jon Snow
    # | jq '.'  : formats and indents the raw JSON for easier reading

# API Authentication
curl -s "http://newsapi.org/v2/everything?q=linux&apiKey=$(< /data/.secret/newsapi.org_apikey)" |  jq '.' | trim 30


# streaming APIs
curl -s "https://stream.wikimedia.org/v2/stream/recentchange" | sample -s 10 > wikimedia-stream-sample
    # take a 10 second sample of one of Wikimedia’s streaming APIs


# sed -- stream editor
< wikimedia-stream-sample sed -n 's/^data: //p' | jq 'select(.type == "edit" and .server_name == "en.wikipedia.org") | .title'
    # This sed expression only prints lines that start with data: and prints the part after the semicolon, which happens to be JSON.
    # This jq expression prints the title key of JSON objects that have a certain type and server_name.

# fun with telnet
telnet starwarstel.net



#######################

# Chapter 4 Creating Command-line Tools
curl -sL "https://www.gutenberg.org/files/11/11-0.txt" |
tr '[:upper:]' '[:lower:]' |
grep -oE "[a-z\']{2,}" |
sort |
uniq -c |
sort -nr | 
head -n 10
    # Downloading an ebook using curl.
    # Converting the entire text to lowercase using tr47.
    # Extracting all the words using grep48 and put each word on separate line.
    # Sort these words in alphabetical order using sort49.
    # Remove all the duplicates and count how often each word appears in the list using uniq50.
    # Sort this list of unique words by their count in descending order using sort.
    # Keep only the top 10 lines (i.e., words) using head.

# 
curl -sL "https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/
stopwords-en.txt" |
sort | tee stopwords | trim 20


#
curl -sL "https://www.gutenberg.org/files/11/11-0.txt" |
tr '[:upper:]' '[:lower:]' |
grep -oE "[a-z\']{2,}" |
sort |
grep -Fvwf stopwords |
uniq -c |
sort -nr |
head -n 10
     # Obtain the patterns from a file (stopwords in our case), one per line, with -f. 
    # Interpret those patterns as fixed strings with -F. Select only those lines containing matches that form whole words with -w. 
    # Select non-matching lines with -v

## text edditor
fc 

## chmod giving permission to execute

cp -v top-words-{1,2}.sh
 
chmod u+x top-words-2.sh
    # The argument u+x consists of three characters: 
    # (1) u indicates that we want to change the permissions for the user who owns the file,
    #  which is you, because you created the file; 
    # (2) + indicates that we want to add a permission; and 
    # (3) x, which indicates the permissions to execute.

##  Define Shebang
 #!/usr/bin/env bash

## Remove Fixed Input

sed -i '2d' top-words-4.sh

    # sed: Stream editor for filtering and transforming text.
    # -i: Edits the file in place, saving the changes directly to top-words-4.sh.
    # '2d': Instructs sed to delete (d) the second line (2) of the file.

## add arguments
#!/usr/bin/env bash
NUM_WORDS="${1:-10}"
tr '[:upper:]' '[:lower:]' |
grep -oE "[a-z\']{2,}" |
sort |
grep -Fvwf stopwords |
uniq -c |
sort -nr |
head -n "${NUM_WORDS}"
    # The variable NUM_WORDS is set to the value of $1, which is a special variable in Bash. 
    # It holds the value of the first command-line argument passed to our command-line tool. 
    # The table below lists the other special variables that Bash offers. If no value is specified, it will take on the value “10.”
    # Note that in order to use the value of the $NUM_WORDS variable, 
    # you need to put a dollar sign in front of it. When you set it, you don’t write a dollar sign.


## Extend Your PATH
echo $PATH
echo $PATH | tr ':' '\n'
cp -v top-words{-5.sh,}
export PATH="${PATH}:/data/data/ch04"
< alice.txt top-words 10


# python and R
time < alice.txt top-words 5
time < alice.txt top-words.py 5
time < alice.txt top-words.R 5



##########################

# Chapter 5: Scrubbing Data

#transformation


grep -E "fizz|buzz" fb.seq | sort | uniq -c | sort -nr > fb.cnt
< fb.cnt awk 'BEGIN { print "value,count" } { print $2","$1 }' > fb.csv
rush plot -x value -y count --geom col --height 2 fb.csv > fb.png
    #rush to run r script from terminal. developed by the author of this book

# Filtering Lines
seq -f "Line %g" 10 | tee lines

< lines head -n 3
< lines sed -n '1,3p'
< lines awk 'NR <= 3'
    # can print the first 3 lines using either head, sed, or awk


< lines tail -n 3
 # tail

< lines tail -n +4
< lines sed '1,3d'
< lines sed -n '1,3!p'
    # removing first three lines

< lines head -n -3
    # remove last three lines

< lines sed -n '4,6p'
< lines awk '(NR>=4) && (NR<=6)'
< lines head -n 6 | tail -n 3
    # print specific lines using a either sed, awk, or a combination of head and tail



< lines sed -n '1~2p'
< lines awk 'NR%2'
    # print odd lines with sed by specifying a start and a step, or with awk by using the modulo operator



< lines sed -n '0~2p'
< lines awk '(NR+1)%2'
    # Printing even lines works in a similar manner


# pattern with grepl
< alice.txt grep -i chapter
 #The -i options specifies that the matching should be case-insensitive.

 < alice.txt grep -E '^CHAPTER (.*)\. The'
    # print the headings that start with The
    # specify the -E option in order to enable regular expressions

< alice.txt grep -Ev '^\s$' | wc -l
    # With the -v option you invert the matches
    # The regular expression '^\s$' matches lines that contain white space, only
    #  using wc -l, you can count the number of non-empty lines


# sample
seq -f "Line %g" 1000 | sample -r 1%

# Extracting Values
grep -i chapter alice.txt | cut -d ' ' -f 3-
    #Here, each line that’s passed to cut is being split on spaces into fields, and then the third field to the last field is being printed.
sed -rn 's/^CHAPTER ([IVXLCDM]{1,})\. (.*)$/\2/p' alice.txt | trim 3
grep -i chapter alice.txt | cut -c 9-

< alice.txt grep -oE '\w{2,}' | trim


< alice.txt tr '[:upper:]' '[:lower:]' | grep -oE '\w{2,}' |grep -E '^a.*e$' | sort | uniq | sort -nr | trim
    # words that start with an a and end with an e

# replacing and deleting values
echo 'hello world!' | tr ' ' '_'
    #using tr

echo 'hello world!' | tr ' !' '_?'
    #more than one character replaced


echo 'hello world!' | tr -d ' !'
    # delete characters

echo 'hello world!' | tr '[a-z]' '[A-Z]'
echo 'hello world!' | tr '[:lower:]' '[:upper:]'
    # convert text to uppercase


echo 'hallo wêreld!' | tr '[:lower:]' '[:upper:]'
echo 'hallo wêreld!' | sed 's/[[:lower:]]*/\U&/g'
    # use 'sed' for non ASCII characters

Rscript -e 'args <- commandArgs(trailingOnly=T); cat(toupper(args[1]), "!\n")' 'hallo wêreld!'
    # R


echo ' hello     world!' |
sed -re 's/hello/bye/' | 
sed -re 's/\s+/ /g' | 
sed -re 's/\s+//'

    # sed -re 's/hello/bye/' Replace hello with bye.
    # sed -re 's/\s+/ /g' Replace any whitespace with one space. The flag g stands for global, meaning that the same substitution can be applied more than once on the same line.
    # sed -re 's/\s+//' This only removes leading spaces because I didn’t specify the flag g here.


echo ' hello     world!' |
sed -re 's/hello/bye/;s/\s+/ /g;s/\s+//'
    # same thing in one sed command



# CSV

# body, header and cols (command line tools -- comes from dsutils)

echo -e "value\n7\n2\n5\n3" | body sort -n
    # It works like this:
    # Take one line from standard in and store it as a variable named $header.
    # Print out the header.
    # Execute all the command-line arguments passed to body on the remaining data in standard in.

